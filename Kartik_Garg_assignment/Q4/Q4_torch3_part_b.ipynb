{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 50000 of 100000\n",
      "   Avg. loss:   2.62\n",
      "    Hit Rate:   0.25\n",
      "   Mentis : Greek == Greek\n",
      "iteration # 100000 of 100000\n",
      "   Avg. loss:   2.04\n",
      "    Hit Rate:   0.38\n",
      "   Mai : Chinese != Vietnamese\n",
      " \n",
      "Classification Accuracy: 37.93%\n",
      " \n",
      "Confusion Matrix\n",
      "      |----------------------- Predicted ------------------------- ...\n",
      "Actual|Czech |German|Arabic|Japane|Chines|Vietna|Russia|French|Irish |Englis|Indian|Pakist|Spanis|Greek |Italia|Portug|Scotti|Dutch |Korean|Polish|\n",
      "======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|\n",
      "Czech |   760|   840|   560|   580|   140|    80|   240|   440|   280|    80|   260|    20|   680|   460|   560|   240|   840|   360|   100|  2860|\n",
      "German|   160|  4780|   840|   240|   380|   220|    60|  1020|   420|   380|   460|     0|   660|   260|   480|   380|  1620|  1060|   180|   880|\n",
      "Arabic|   720|  1600| 19840|  4400|   660|   500|     0|  1480|   440|     0|  2500|  1020|  1080|  1800|   640|   480|  1080|     0|  1340|   420|\n",
      "Japane|   180|     0|  1160| 11280|   360|   460|   180|   140|   100|    60|   400|   620|   280|   260|  1680|   260|   280|    60|   340|  1720|\n",
      "Chines|     0|    40|     0|     0|  3380|   820|     0|     0|     0|    40|   140|     0|     0|     0|     0|     0|     0|    20|   900|    20|\n",
      "Vietna|     0|     0|     0|     0|   400|   860|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|    20|   180|     0|\n",
      "Russia|  1400|  3700|  6760|  3360|   740|  1160| 83060|  7460|  8680|   940|  1180|   740|  2400| 16560|  7760|  1260| 11940|  3240|  1240| 24580|\n",
      "French|    20|   400|   460|    20|    80|     0|    20|  1940|   220|   140|   180|    20|   100|   260|   340|   240|   960|    60|    60|    20|\n",
      "Irish |    60|    60|   260|    40|    60|    20|    80|   280|  2280|   140|   100|     0|   160|   120|   160|    40|   620|   100|    40|    20|\n",
      "Englis|  1340|  5680|  4060|   720|  1220|  1100|  1680|  8240|  5360|  5260|  2180|   220|  1760|  3700|  3160|  2860| 16480|  4420|  1140|  2780|\n",
      "Indian|    20|    80|   320|   100|   180|    40|    20|    40|    80|     0|   600|   120|   100|    40|   100|    40|    40|    40|    20|    20|\n",
      "Pakist|    80|     0|   900|   480|   220|   160|   100|   100|   240|     0|   440|   820|   160|   120|   640|   120|    60|    20|    60|   140|\n",
      "Spanis|   100|   280|   180|   140|    40|    20|     0|   220|    60|    20|   180|     0|  1880|   500|  1220|   680|   240|    60|    80|    60|\n",
      "Greek |     0|     0|   120|    60|     0|     0|     0|    20|     0|     0|    20|     0|    20|  3400|    80|    40|   220|    40|     0|    40|\n",
      "Italia|   180|    60|   340|   240|   200|    80|    20|   620|    80|   100|   280|   280|  1200|   200|  8860|  1060|   260|     0|    20|   100|\n",
      "Portug|    40|     0|    60|     0|     0|     0|     0|   140|    40|     0|    20|     0|   260|   120|   280|   420|     0|   100|     0|     0|\n",
      "Scotti|     0|   100|    80|     0|    80|    80|    40|   140|    80|    60|    40|     0|     0|    40|    40|    40|  1060|    80|    20|    20|\n",
      "Dutch |   120|   760|    80|    60|   220|    40|   140|   520|   160|   100|   220|     0|   220|   320|   300|    80|   440|  1640|   100|   420|\n",
      "Korean|    20|    60|    40|     0|   580|   220|     0|     0|     0|     0|    40|     0|     0|     0|     0|     0|     0|     0|   880|    40|\n",
      "Polish|    60|   120|    20|   100|    40|     0|    20|    60|    60|    20|    60|     0|    60|    40|    60|    20|    80|    60|    20|  1880|\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import glob\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "\n",
    "# various helper functions\n",
    "from torch_name_classifier_helpers import readLines\n",
    "from torch_name_classifier_helpers import randomTrainingExample\n",
    "from torch_name_classifier_helpers import categoryFromOutput\n",
    "from torch_name_classifier_helpers import textToTensor\n",
    "\n",
    "# declare RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_layer, hidden_layer):\n",
    "        combined_layer = torch.cat((input_layer,hidden_layer), 1)\n",
    "        hidden_layer = self.i2h(combined_layer)\n",
    "        output_layer = self.i2o(combined_layer)\n",
    "        output_layer = self.softmax(output_layer)\n",
    "        return output_layer, hidden_layer\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,self.hidden_size)\n",
    "\n",
    "# develop training function\n",
    "def train(the_rnn, category_tensor, line_tensor, criterion, learn_rate):\n",
    "    # init hidden layer\n",
    "    hidden_layer = the_rnn.initHidden()\n",
    "    the_rnn.zero_grad()\n",
    "\n",
    "    # make out predictions, one char at a time\n",
    "    for i in range(line_tensor.size(0)):\n",
    "        output_layer,hidden_layer = the_rnn(line_tensor[i], hidden_layer)\n",
    "\n",
    "    loss = criterion(output_layer, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # use steepest-descent to optimize\n",
    "    for p in the_rnn.parameters():\n",
    "        p.data.add_(-learn_rate, p.grad.data)\n",
    "\n",
    "    return output_layer,loss.item()\n",
    "\n",
    "def predict(the_rnn, line_tensor):\n",
    "    hidden_layer = the_rnn.initHidden()\n",
    "\n",
    "    for i in range(list(line_tensor.size())[0]):\n",
    "        output_layer,hidden_layer = the_rnn(line_tensor[i], hidden_layer)\n",
    "\n",
    "    return output_layer\n",
    "        \n",
    "def main():\n",
    "    # declare regex for files containing names\n",
    "    fnames = 'data/names/*.txt'\n",
    "\n",
    "    # assemble sequence of valid ASCII characters\n",
    "    # that can occur in a name\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    n_letters = len(all_letters)\n",
    "\n",
    "    # Build the category_lines dictionary, a list of names per language\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "\n",
    "    nfiles = 0\n",
    "    for filename in glob.glob(fnames):\n",
    "        # basename of file is the lanquage\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        # add category (i.e. language) to list\n",
    "        all_categories.append(category)\n",
    "        # add names to dictionary, indexed by language\n",
    "        lines = readLines(filename, all_letters)\n",
    "        category_lines[category] = lines\n",
    "        nfiles += 1\n",
    "    if(nfiles == 0):\n",
    "        print(\"No files found for regular expression (\"+fnames+\")\")\n",
    "        sys.exit(-1)\n",
    "        \n",
    "    # count number of languages (i.e. classes)\n",
    "    n_categories = len(all_categories)\n",
    "\n",
    "    # write categories to csv file\n",
    "    with open('all_categories.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([all_categories])\n",
    "\n",
    "    # 4. create instance of the RNN\n",
    "    n_input_neurons = n_letters\n",
    "    n_hidden_neurons = 256\n",
    "    n_output_neurons = n_categories\n",
    "    MyRNN = RNN(n_input_neurons, n_hidden_neurons, n_output_neurons)\n",
    "    \n",
    "    # 5. load checkpoint, if available\n",
    "    \n",
    "    # 6. set training parameters\n",
    "    my_criterion = nn.NLLLoss()\n",
    "    my_learn_rate = 0.001\n",
    "    n_iters = 100000\n",
    "    print_every = 50000\n",
    "    avg_loss = 0.00\n",
    "    hit_rate = 0.00\n",
    "    dhit = 1.00 / float(print_every)\n",
    "    all_avg_losses = []\n",
    "    all_hit_rates = []\n",
    "    \n",
    "    for i in range(1, n_iters + 1):\n",
    "# TO DO:\n",
    "        # 7. train on random feature-label pair\n",
    "        category, line, category_tensor, line_tensor =\\\n",
    "                  randomTrainingExample(all_categories, category_lines, all_letters)\n",
    "        output, loss = train(MyRNN, category_tensor, line_tensor, my_criterion, my_learn_rate)\n",
    "        avg_loss += loss\n",
    "        guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "\n",
    "        if(guess == category):\n",
    "            hit_rate += dhit\n",
    "            \n",
    "        # periodically report progress\n",
    "        if i % print_every == 0:\n",
    "            avg_loss /= float(print_every)\n",
    "            print(\"iteration # \" + str(i) + \" of \" + str(n_iters))            \n",
    "            print(\"   Avg. loss: {:6.2f}\".format(avg_loss))\n",
    "            print(\"    Hit Rate: {:6.2f}\".format(hit_rate))\n",
    "            if(guess == category):\n",
    "                print(\"   \" + line + \" : \" + guess + \" == \" + category)\n",
    "            else:\n",
    "                print(\"   \" + line + \" : \" + guess + \" != \" + category)\n",
    "            all_avg_losses.append(avg_loss)\n",
    "            all_hit_rates.append(hit_rate)\n",
    "            avg_loss = 0.00\n",
    "            hit_rate = 0.00\n",
    "            \n",
    "        # 8. Save a checkpoint\n",
    "            \n",
    "    fig,ax1 = plt.subplots()\n",
    "    ax1.plot(all_avg_losses)\n",
    "    ax1.set_ylabel(\"Loss Function\", color=\"r\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(all_hit_rates)\n",
    "    ax2.set_ylabel(\"Success Rate\", color=\"b\")\n",
    "    plt.xlabel(\"Training Iteration\")\n",
    "    plt.savefig(\"convergence.png\", dpi=100)\n",
    "    # plt.show()    \n",
    "    plt.close()\n",
    "\n",
    "    # test the skill of the model\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    confusion_matrix = np.zeros((n_categories,n_categories))\n",
    "    for actual_category in all_categories:\n",
    "        cat_idx = all_categories.index(actual_category)\n",
    "        for line in category_lines[actual_category]:\n",
    "            line_tensor = textToTensor(line, all_letters)\n",
    "            output = predict(MyRNN, line_tensor)\n",
    "            guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "            \n",
    "            # update confusion matrix\n",
    "            for i in range(0,n_categories):\n",
    "                confusion_matrix[cat_idx,guess_idx] += 1          \n",
    "            total += 1\n",
    "            correct += int(cat_idx == guess_idx)\n",
    "\n",
    "    # report results\n",
    "    print(' ')\n",
    "    print('Classification Accuracy: {:.2f}%'.format(100 * correct / total))\n",
    "    print(' ')\n",
    "    print('Confusion Matrix')\n",
    "    print('      |----------------------- Predicted ------------------------- ...')\n",
    "    print('Actual|',end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "    print('')\n",
    "    print('======|', end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('======|', end='')\n",
    "    print('')          \n",
    "    for i in range(0,n_categories):\n",
    "        print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "        for j in range(0,n_categories):\n",
    "            cm = int(confusion_matrix[i,j])\n",
    "            print('{:6d}|'.format(cm),end='')\n",
    "        print('')\n",
    "\n",
    "# launch the main program\n",
    "main()\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 100000 of 200000\n",
      "   Avg. loss:   2.34\n",
      "    Hit Rate:   0.32\n",
      "   Araullo : Italian != Portuguese\n",
      "iteration # 200000 of 200000\n",
      "   Avg. loss:   1.65\n",
      "    Hit Rate:   0.50\n",
      "   Kouches : Greek == Greek\n",
      " \n",
      "Classification Accuracy: 57.04%\n",
      " \n",
      "Confusion Matrix\n",
      "      |----------------------- Predicted ------------------------- ...\n",
      "Actual|Czech |German|Arabic|Japane|Chines|Vietna|Russia|French|Irish |Englis|Indian|Pakist|Spanis|Greek |Italia|Portug|Scotti|Dutch |Korean|Polish|\n",
      "======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|\n",
      "Czech |  3380|   980|   520|   620|    40|   140|   860|   100|    60|   420|   440|   220|   280|   500|    80|   180|   340|   300|    60|   860|\n",
      "German|   820|  6100|   720|   380|   300|   180|   700|   380|   120|  1040|   380|   180|   260|   400|   180|   220|   860|   840|   220|   200|\n",
      "Arabic|   660|   760| 27440|  3040|  1200|   800|   760|     0|     0|   420|   560|  1260|     0|  1320|   300|   440|   420|   360|   260|     0|\n",
      "Japane|   260|    60|   780| 15380|   220|   200|   420|    60|     0|    80|   320|   700|    40|   180|   480|   120|     0|    60|   280|   180|\n",
      "Chines|     0|     0|    20|    20|  3400|   760|     0|     0|     0|     0|    80|    40|     0|     0|    20|     0|     0|     0|  1000|    20|\n",
      "Vietna|     0|     0|     0|     0|   140|  1200|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|   120|     0|\n",
      "Russia|  5280|  2800|  4820|  4380|   860|  1100|131780|  1280|  2960|  2840|  1180|  3260|  1200| 12480|  1240|   720|  2460|  2080|   980|  4460|\n",
      "French|   100|   460|   420|    60|    80|    20|   320|  1700|   200|   580|   260|    60|   120|   520|   180|   100|   180|    80|   100|     0|\n",
      "Irish |   100|    60|   200|   160|    40|    40|   320|   180|  2240|   320|   120|   180|    80|    80|    20|    60|   300|    60|    60|    20|\n",
      "Englis|  3580|  7040|  3680|  1760|  1220|  1360|  4800|  3820|  3580| 16660|  1660|  1760|  1360|  4400|   860|  1500|  8380|  3520|  1320|  1100|\n",
      "Indian|    20|    20|   120|    80|    80|   100|    40|    40|    20|     0|   860|   360|    40|    20|    20|    40|     0|    60|    80|     0|\n",
      "Pakist|   180|    40|   420|   400|   220|    80|   100|    20|     0|    60|   460|  2220|   100|    80|   240|    40|    40|    20|    80|    60|\n",
      "Spanis|   220|   240|   200|   220|    80|    40|   100|   200|    80|    60|   180|    60|  2260|   540|   720|   400|    80|   160|   100|    20|\n",
      "Greek |    40|     0|    40|    60|     0|     0|    20|     0|     0|     0|     0|     0|     0|  3760|    80|    40|     0|    20|     0|     0|\n",
      "Italia|   480|    80|   360|   520|   140|   120|   220|   580|   100|    80|   420|   560|  1480|   440|  7600|   660|   220|    40|    20|    60|\n",
      "Portug|    40|     0|   120|    40|     0|     0|    20|    40|     0|     0|    40|    20|   300|   160|    40|   580|     0|    60|    20|     0|\n",
      "Scotti|     0|    80|    80|     0|    80|    20|   100|     0|    20|   220|    20|    40|    20|    60|    40|    20|  1140|    40|     0|    20|\n",
      "Dutch |   400|   540|   240|    80|   140|   120|   460|   100|   160|   420|   220|    60|   140|   220|   120|    40|   100|  2220|   100|    60|\n",
      "Korean|    20|     0|    20|     0|   380|   120|     0|     0|     0|     0|    20|     0|     0|     0|     0|     0|     0|     0|  1300|    20|\n",
      "Polish|   440|    60|    20|   100|    40|     0|    80|    20|    60|    80|    20|     0|    40|    60|     0|     0|    40|    20|    20|  1680|\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # declare regex for files containing names\n",
    "    fnames = 'data/names/*.txt'\n",
    "\n",
    "    # assemble sequence of valid ASCII characters\n",
    "    # that can occur in a name\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    n_letters = len(all_letters)\n",
    "\n",
    "    # Build the category_lines dictionary, a list of names per language\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "\n",
    "    nfiles = 0\n",
    "    for filename in glob.glob(fnames):\n",
    "        # basename of file is the lanquage\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        # add category (i.e. language) to list\n",
    "        all_categories.append(category)\n",
    "        # add names to dictionary, indexed by language\n",
    "        lines = readLines(filename, all_letters)\n",
    "        category_lines[category] = lines\n",
    "        nfiles += 1\n",
    "    if(nfiles == 0):\n",
    "        print(\"No files found for regular expression (\"+fnames+\")\")\n",
    "        sys.exit(-1)\n",
    "        \n",
    "    # count number of languages (i.e. classes)\n",
    "    n_categories = len(all_categories)\n",
    "\n",
    "    # write categories to csv file\n",
    "    with open('all_categories.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([all_categories])\n",
    "\n",
    "    # 4. create instance of the RNN\n",
    "    n_input_neurons = n_letters\n",
    "    n_hidden_neurons = 256\n",
    "    n_output_neurons = n_categories\n",
    "    MyRNN = RNN(n_input_neurons, n_hidden_neurons, n_output_neurons)\n",
    "    \n",
    "    # 5. load checkpoint, if available\n",
    "    \n",
    "    # 6. set training parameters\n",
    "    my_criterion = nn.NLLLoss()\n",
    "    my_learn_rate = 0.001\n",
    "    n_iters = 200000\n",
    "    print_every = 100000\n",
    "    avg_loss = 0.00\n",
    "    hit_rate = 0.00\n",
    "    dhit = 1.00 / float(print_every)\n",
    "    all_avg_losses = []\n",
    "    all_hit_rates = []\n",
    "    \n",
    "    for i in range(1, n_iters + 1):\n",
    "        category, line, category_tensor, line_tensor =\\\n",
    "                  randomTrainingExample(all_categories, category_lines, all_letters)\n",
    "        output, loss = train(MyRNN, category_tensor, line_tensor, my_criterion, my_learn_rate)\n",
    "        avg_loss += loss\n",
    "        guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "\n",
    "        if(guess == category):\n",
    "            hit_rate += dhit\n",
    "            \n",
    "        # periodically report progress\n",
    "        if i % print_every == 0:\n",
    "            avg_loss /= float(print_every)\n",
    "            print(\"iteration # \" + str(i) + \" of \" + str(n_iters))            \n",
    "            print(\"   Avg. loss: {:6.2f}\".format(avg_loss))\n",
    "            print(\"    Hit Rate: {:6.2f}\".format(hit_rate))\n",
    "            if(guess == category):\n",
    "                print(\"   \" + line + \" : \" + guess + \" == \" + category)\n",
    "            else:\n",
    "                print(\"   \" + line + \" : \" + guess + \" != \" + category)\n",
    "            all_avg_losses.append(avg_loss)\n",
    "            all_hit_rates.append(hit_rate)\n",
    "            avg_loss = 0.00\n",
    "            hit_rate = 0.00\n",
    "            \n",
    "        # 8. Save a checkpoint\n",
    "            \n",
    "    fig,ax1 = plt.subplots()\n",
    "    ax1.plot(all_avg_losses)\n",
    "    ax1.set_ylabel(\"Loss Function\", color=\"r\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(all_hit_rates)\n",
    "    ax2.set_ylabel(\"Success Rate\", color=\"b\")\n",
    "    plt.xlabel(\"Training Iteration\")\n",
    "    plt.savefig(\"convergence.png\", dpi=100)\n",
    "    # plt.show()    \n",
    "    plt.close()\n",
    "\n",
    "    # test the skill of the model\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    confusion_matrix = np.zeros((n_categories,n_categories))\n",
    "    for actual_category in all_categories:\n",
    "        cat_idx = all_categories.index(actual_category)\n",
    "        for line in category_lines[actual_category]:\n",
    "            line_tensor = textToTensor(line, all_letters)\n",
    "            output = predict(MyRNN, line_tensor)\n",
    "            guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "            \n",
    "            # update confusion matrix\n",
    "            for i in range(0,n_categories):\n",
    "                confusion_matrix[cat_idx,guess_idx] += 1          \n",
    "            total += 1\n",
    "            correct += int(cat_idx == guess_idx)\n",
    "\n",
    "    # report results\n",
    "    print(' ')\n",
    "    print('Classification Accuracy: {:.2f}%'.format(100 * correct / total))\n",
    "    print(' ')\n",
    "    print('Confusion Matrix')\n",
    "    print('      |----------------------- Predicted ------------------------- ...')\n",
    "    print('Actual|',end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "    print('')\n",
    "    print('======|', end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('======|', end='')\n",
    "    print('')          \n",
    "    for i in range(0,n_categories):\n",
    "        print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "        for j in range(0,n_categories):\n",
    "            cm = int(confusion_matrix[i,j])\n",
    "            print('{:6d}|'.format(cm),end='')\n",
    "        print('')\n",
    "\n",
    "# launch the main program\n",
    "main()\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 250000 of 500000\n",
      "   Avg. loss:   1.88\n",
      "    Hit Rate:   0.44\n",
      "   Cowie : Greek != English\n",
      "iteration # 500000 of 500000\n",
      "   Avg. loss:   1.24\n",
      "    Hit Rate:   0.61\n",
      "   Lindsay : English != Scottish\n",
      " \n",
      "Classification Accuracy: 62.56%\n",
      " \n",
      "Confusion Matrix\n",
      "      |----------------------- Predicted ------------------------- ...\n",
      "Actual|Czech |German|Arabic|Japane|Chines|Vietna|Russia|French|Irish |Englis|Indian|Pakist|Spanis|Greek |Italia|Portug|Scotti|Dutch |Korean|Polish|\n",
      "======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|\n",
      "Czech |  3460|   740|   460|   300|    60|   200|   380|   220|   500|   900|   360|   360|   240|   200|   260|   260|   240|   540|   100|   600|\n",
      "German|   420|  6520|   620|   100|   180|   220|   260|   720|   540|  1340|   480|   200|   100|   100|   220|   320|   800|   860|   300|   180|\n",
      "Arabic|     0|     0| 32020|   680|  1200|   240|     0|     0|   400|   480|  1340|   380|     0|  1040|   740|   440|   420|   360|   260|     0|\n",
      "Japane|   100|    20|  1260| 15200|   180|   220|   120|    60|   140|    80|   560|   540|    40|    80|   340|   320|    80|   180|   220|    80|\n",
      "Chines|     0|     0|    20|     0|  3460|   600|     0|     0|     0|     0|    80|    20|     0|     0|    20|    40|     0|     0|  1080|    40|\n",
      "Vietna|     0|     0|     0|     0|   140|  1260|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|    60|     0|\n",
      "Russia|  2860|  2800|  5640|  3620|   960|  1220|132580|  2140|  5860|  7380|  1800|  5220|   860|  3260|  1780|  1400|  3120|  1760|  1300|  2600|\n",
      "French|    20|   300|   200|     0|    20|    40|   100|  2680|   540|   520|   240|   160|    40|   120|   160|   120|   140|    40|   100|     0|\n",
      "Irish |    40|    40|   200|    40|    20|    20|    20|    40|  3460|   140|    60|   100|    20|     0|     0|    60|   240|    80|    60|     0|\n",
      "Englis|  1500|  5060|  3760|   880|   940|  1220|  1300|  3660|  8000| 26920|  2040|  1580|   700|  1300|  1140|  1840|  6400|  3000|  1420|   700|\n",
      "Indian|    20|    20|    60|    20|    60|    60|     0|    40|    40|     0|  1260|   200|     0|     0|    20|    60|     0|    40|    80|    20|\n",
      "Pakist|    80|    20|   420|   100|   160|   120|    40|    40|   140|    20|   380|  2860|    20|    20|   240|    60|    60|    20|    40|    20|\n",
      "Spanis|    60|   100|   160|    40|    40|    40|    40|   160|   300|    80|   300|    80|  2200|   140|   780|  1120|   100|   140|    20|    60|\n",
      "Greek |    20|     0|    80|    60|     0|     0|    40|    20|     0|    60|     0|     0|    40|  3540|    40|   140|     0|    20|     0|     0|\n",
      "Italia|   200|    80|   320|   340|    80|   120|    40|   420|   260|   100|   540|   620|   600|    40|  9200|   900|   180|    60|    20|    60|\n",
      "Portug|    40|     0|    20|     0|     0|     0|    20|     0|    40|     0|     0|     0|    40|     0|    60|  1220|     0|    40|     0|     0|\n",
      "Scotti|     0|   100|    20|     0|    80|    20|     0|     0|   240|   200|    40|     0|    20|    40|    40|    20|  1120|    40|     0|    20|\n",
      "Dutch |   120|   340|   280|    60|   100|   100|   100|   220|   220|   460|   140|   100|    40|   160|   120|    80|   180|  3000|   120|     0|\n",
      "Korean|     0|     0|     0|     0|   220|   120|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|  1520|    20|\n",
      "Polish|   100|    40|    40|    40|    40|     0|    80|    20|   120|   120|    40|     0|    40|    40|     0|     0|    40|    20|    40|  1960|\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # declare regex for files containing names\n",
    "    fnames = 'data/names/*.txt'\n",
    "\n",
    "    # assemble sequence of valid ASCII characters\n",
    "    # that can occur in a name\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    n_letters = len(all_letters)\n",
    "\n",
    "    # Build the category_lines dictionary, a list of names per language\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "\n",
    "    nfiles = 0\n",
    "    for filename in glob.glob(fnames):\n",
    "        # basename of file is the lanquage\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        # add category (i.e. language) to list\n",
    "        all_categories.append(category)\n",
    "        # add names to dictionary, indexed by language\n",
    "        lines = readLines(filename, all_letters)\n",
    "        category_lines[category] = lines\n",
    "        nfiles += 1\n",
    "    if(nfiles == 0):\n",
    "        print(\"No files found for regular expression (\"+fnames+\")\")\n",
    "        sys.exit(-1)\n",
    "        \n",
    "    # count number of languages (i.e. classes)\n",
    "    n_categories = len(all_categories)\n",
    "\n",
    "    # write categories to csv file\n",
    "    with open('all_categories.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([all_categories])\n",
    "\n",
    "    # 4. create instance of the RNN\n",
    "    n_input_neurons = n_letters\n",
    "    n_hidden_neurons = 256\n",
    "    n_output_neurons = n_categories\n",
    "    MyRNN = RNN(n_input_neurons, n_hidden_neurons, n_output_neurons)\n",
    "    \n",
    "    # 5. load checkpoint, if available\n",
    "    \n",
    "    # 6. set training parameters\n",
    "    my_criterion = nn.NLLLoss()\n",
    "    my_learn_rate = 0.001\n",
    "    n_iters = 500000\n",
    "    print_every = 250000\n",
    "    avg_loss = 0.00\n",
    "    hit_rate = 0.00\n",
    "    dhit = 1.00 / float(print_every)\n",
    "    all_avg_losses = []\n",
    "    all_hit_rates = []\n",
    "    \n",
    "    for i in range(1, n_iters + 1):\n",
    "        category, line, category_tensor, line_tensor =\\\n",
    "                  randomTrainingExample(all_categories, category_lines, all_letters)\n",
    "        output, loss = train(MyRNN, category_tensor, line_tensor, my_criterion, my_learn_rate)\n",
    "        avg_loss += loss\n",
    "        guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "\n",
    "        if(guess == category):\n",
    "            hit_rate += dhit\n",
    "            \n",
    "        # periodically report progress\n",
    "        if i % print_every == 0:\n",
    "            avg_loss /= float(print_every)\n",
    "            print(\"iteration # \" + str(i) + \" of \" + str(n_iters))            \n",
    "            print(\"   Avg. loss: {:6.2f}\".format(avg_loss))\n",
    "            print(\"    Hit Rate: {:6.2f}\".format(hit_rate))\n",
    "            if(guess == category):\n",
    "                print(\"   \" + line + \" : \" + guess + \" == \" + category)\n",
    "            else:\n",
    "                print(\"   \" + line + \" : \" + guess + \" != \" + category)\n",
    "            all_avg_losses.append(avg_loss)\n",
    "            all_hit_rates.append(hit_rate)\n",
    "            avg_loss = 0.00\n",
    "            hit_rate = 0.00\n",
    "            \n",
    "        # 8. Save a checkpoint\n",
    "        torch.save(MyRNN.state_dict(),'mnist_names_model.pkl')\n",
    "            \n",
    "    fig,ax1 = plt.subplots()\n",
    "    ax1.plot(all_avg_losses)\n",
    "    ax1.set_ylabel(\"Loss Function\", color=\"r\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(all_hit_rates)\n",
    "    ax2.set_ylabel(\"Success Rate\", color=\"b\")\n",
    "    plt.xlabel(\"Training Iteration\")\n",
    "    plt.savefig(\"convergence.png\", dpi=100)\n",
    "    # plt.show()    \n",
    "    plt.close()\n",
    "\n",
    "    # test the skill of the model\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    confusion_matrix = np.zeros((n_categories,n_categories))\n",
    "    for actual_category in all_categories:\n",
    "        cat_idx = all_categories.index(actual_category)\n",
    "        for line in category_lines[actual_category]:\n",
    "            line_tensor = textToTensor(line, all_letters)\n",
    "            output = predict(MyRNN, line_tensor)\n",
    "            guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "            \n",
    "            # update confusion matrix\n",
    "            for i in range(0,n_categories):\n",
    "                confusion_matrix[cat_idx,guess_idx] += 1          \n",
    "            total += 1\n",
    "            correct += int(cat_idx == guess_idx)\n",
    "\n",
    "    # report results\n",
    "    print(' ')\n",
    "    print('Classification Accuracy: {:.2f}%'.format(100 * correct / total))\n",
    "    print(' ')\n",
    "    print('Confusion Matrix')\n",
    "    print('      |----------------------- Predicted ------------------------- ...')\n",
    "    print('Actual|',end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "    print('')\n",
    "    print('======|', end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('======|', end='')\n",
    "    print('')          \n",
    "    for i in range(0,n_categories):\n",
    "        print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "        for j in range(0,n_categories):\n",
    "            cm = int(confusion_matrix[i,j])\n",
    "            print('{:6d}|'.format(cm),end='')\n",
    "        print('')\n",
    "\n",
    "# launch the main program\n",
    "main()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 50000 of 100000\n",
      "   Avg. loss:   2.01\n",
      "    Hit Rate:   0.40\n",
      "   Macshuibhne : Dutch != Irish\n",
      "iteration # 100000 of 100000\n",
      "   Avg. loss:   1.48\n",
      "    Hit Rate:   0.54\n",
      "   Schirmer : German == German\n",
      " \n",
      "Classification Accuracy: 43.28%\n",
      " \n",
      "Confusion Matrix\n",
      "      |----------------------- Predicted ------------------------- ...\n",
      "Actual|Czech |German|Arabic|Japane|Chines|Vietna|Russia|French|Irish |Englis|Indian|Pakist|Spanis|Greek |Italia|Portug|Scotti|Dutch |Korean|Polish|\n",
      "======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|\n",
      "Czech |  2440|  1120|   340|   200|   140|   160|   160|   400|    20|   680|   360|    20|   480|   100|   400|   180|   400|   380|   300|  2100|\n",
      "German|   480|  6060|   400|    40|   180|   240|    20|  1340|   100|   580|   360|     0|   620|   100|   260|   280|  1660|   640|   640|   480|\n",
      "Arabic|   720|   280| 20600|   680|   320|   500|     0|   380|     0|   420|  7200|  1500|  1200|  1500|   300|  1160|   580|   760|  1040|   860|\n",
      "Japane|   580|   200|   800|  7240|   220|   200|    80|   340|    40|   220|  1500|   960|  1040|    80|  1980|   500|   220|   180|   700|  2740|\n",
      "Chines|     0|     0|    20|     0|  3500|   540|     0|     0|     0|     0|    60|    20|     0|     0|    20|    40|     0|     0|  1140|    20|\n",
      "Vietna|     0|     0|     0|     0|   200|  1140|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|   120|     0|\n",
      "Russia|  8960|  7020|  3040|  1100|  1500|  1880| 89540|  6900|  1460|  7080|  1800|   440|  3720|  3720|  3600|  1400| 11640|  2520|  7500| 23340|\n",
      "French|    60|   380|   280|    60|    60|    60|    60|  2960|     0|   300|   140|    40|   100|   320|   140|    20|   240|   100|   180|    40|\n",
      "Irish |    80|   240|   280|    80|   100|    20|   140|   500|   820|   700|   120|     0|   220|    80|   220|   100|   480|   120|   220|   120|\n",
      "Englis|  2180|  7260|  3540|   340|  1140|  1500|   520|  8120|   480| 17200|  1880|   260|  1700|  1980|  1800|  1060| 12880|  3720|  3460|  2340|\n",
      "Indian|    40|    40|    60|    20|    40|    60|    20|    80|    20|    20|  1120|    60|    60|     0|    80|    20|    40|    60|   140|    20|\n",
      "Pakist|    80|    80|   220|    80|   140|    40|     0|   160|    40|    80|   880|  1020|   100|    80|   900|    80|   280|    20|   300|   280|\n",
      "Spanis|   240|    80|   140|     0|    80|    60|     0|   300|     0|   160|   160|     0|  2580|   260|   980|   580|    60|    40|   120|   120|\n",
      "Greek |     0|     0|    60|    80|     0|    20|     0|    80|     0|    40|     0|     0|    80|  3300|    80|   100|    60|    80|     0|    80|\n",
      "Italia|   340|   100|   240|   320|   160|   160|    80|   680|     0|   100|   200|    60|  1220|   240|  9140|   660|    80|    40|   160|   200|\n",
      "Portug|    40|     0|     0|     0|     0|     0|    20|    80|     0|     0|    20|     0|   280|    20|   100|   800|    20|    60|    40|     0|\n",
      "Scotti|    20|   100|    60|     0|    80|     0|     0|   100|     0|   140|     0|     0|     0|    40|    40|     0|  1340|    40|    20|    20|\n",
      "Dutch |   160|   760|   200|    20|   140|   120|     0|   320|    20|   380|   140|     0|   180|   180|    40|   220|   360|  2280|   120|   300|\n",
      "Korean|     0|     0|    20|     0|   180|   120|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|  1520|    40|\n",
      "Polish|   120|    40|    20|     0|    60|    20|     0|    60|     0|    40|    20|     0|   100|    20|     0|    20|    40|    20|    80|  2120|\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # declare regex for files containing names\n",
    "    fnames = 'data/names/*.txt'\n",
    "\n",
    "    # assemble sequence of valid ASCII characters\n",
    "    # that can occur in a name\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    n_letters = len(all_letters)\n",
    "\n",
    "    # Build the category_lines dictionary, a list of names per language\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "\n",
    "    nfiles = 0\n",
    "    for filename in glob.glob(fnames):\n",
    "        # basename of file is the lanquage\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        # add category (i.e. language) to list\n",
    "        all_categories.append(category)\n",
    "        # add names to dictionary, indexed by language\n",
    "        lines = readLines(filename, all_letters)\n",
    "        category_lines[category] = lines\n",
    "        nfiles += 1\n",
    "    if(nfiles == 0):\n",
    "        print(\"No files found for regular expression (\"+fnames+\")\")\n",
    "        sys.exit(-1)\n",
    "        \n",
    "    # count number of languages (i.e. classes)\n",
    "    n_categories = len(all_categories)\n",
    "\n",
    "    # write categories to csv file\n",
    "    with open('all_categories.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([all_categories])\n",
    "\n",
    "    # 4. create instance of the RNN\n",
    "    n_input_neurons = n_letters\n",
    "    n_hidden_neurons = 256\n",
    "    n_output_neurons = n_categories\n",
    "    MyRNN = RNN(n_input_neurons, n_hidden_neurons, n_output_neurons)\n",
    "    \n",
    "    # 5. load checkpoint, if available\n",
    "    \n",
    "    # 6. set training parameters\n",
    "    my_criterion = nn.NLLLoss()\n",
    "    my_learn_rate = 0.005\n",
    "    n_iters = 100000\n",
    "    print_every = 50000\n",
    "    avg_loss = 0.00\n",
    "    hit_rate = 0.00\n",
    "    dhit = 1.00 / float(print_every)\n",
    "    all_avg_losses = []\n",
    "    all_hit_rates = []\n",
    "    \n",
    "    for i in range(1, n_iters + 1):\n",
    "# TO DO:\n",
    "        # 7. train on random feature-label pair\n",
    "        category, line, category_tensor, line_tensor =\\\n",
    "                  randomTrainingExample(all_categories, category_lines, all_letters)\n",
    "        output, loss = train(MyRNN, category_tensor, line_tensor, my_criterion, my_learn_rate)\n",
    "        avg_loss += loss\n",
    "        guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "\n",
    "        if(guess == category):\n",
    "            hit_rate += dhit\n",
    "            \n",
    "        # periodically report progress\n",
    "        if i % print_every == 0:\n",
    "            avg_loss /= float(print_every)\n",
    "            print(\"iteration # \" + str(i) + \" of \" + str(n_iters))            \n",
    "            print(\"   Avg. loss: {:6.2f}\".format(avg_loss))\n",
    "            print(\"    Hit Rate: {:6.2f}\".format(hit_rate))\n",
    "            if(guess == category):\n",
    "                print(\"   \" + line + \" : \" + guess + \" == \" + category)\n",
    "            else:\n",
    "                print(\"   \" + line + \" : \" + guess + \" != \" + category)\n",
    "            all_avg_losses.append(avg_loss)\n",
    "            all_hit_rates.append(hit_rate)\n",
    "            avg_loss = 0.00\n",
    "            hit_rate = 0.00\n",
    "            \n",
    "        # 8. Save a checkpoint\n",
    "            \n",
    "    fig,ax1 = plt.subplots()\n",
    "    ax1.plot(all_avg_losses)\n",
    "    ax1.set_ylabel(\"Loss Function\", color=\"r\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(all_hit_rates)\n",
    "    ax2.set_ylabel(\"Success Rate\", color=\"b\")\n",
    "    plt.xlabel(\"Training Iteration\")\n",
    "    plt.savefig(\"convergence.png\", dpi=100)\n",
    "    # plt.show()    \n",
    "    plt.close()\n",
    "\n",
    "    # test the skill of the model\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    confusion_matrix = np.zeros((n_categories,n_categories))\n",
    "    for actual_category in all_categories:\n",
    "        cat_idx = all_categories.index(actual_category)\n",
    "        for line in category_lines[actual_category]:\n",
    "            line_tensor = textToTensor(line, all_letters)\n",
    "            output = predict(MyRNN, line_tensor)\n",
    "            guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "            \n",
    "            # update confusion matrix\n",
    "            for i in range(0,n_categories):\n",
    "                confusion_matrix[cat_idx,guess_idx] += 1          \n",
    "            total += 1\n",
    "            correct += int(cat_idx == guess_idx)\n",
    "\n",
    "    # report results\n",
    "    print(' ')\n",
    "    print('Classification Accuracy: {:.2f}%'.format(100 * correct / total))\n",
    "    print(' ')\n",
    "    print('Confusion Matrix')\n",
    "    print('      |----------------------- Predicted ------------------------- ...')\n",
    "    print('Actual|',end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "    print('')\n",
    "    print('======|', end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('======|', end='')\n",
    "    print('')          \n",
    "    for i in range(0,n_categories):\n",
    "        print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "        for j in range(0,n_categories):\n",
    "            cm = int(confusion_matrix[i,j])\n",
    "            print('{:6d}|'.format(cm),end='')\n",
    "        print('')\n",
    "\n",
    "# launch the main program\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 100000 of 200000\n",
      "   Avg. loss:   1.75\n",
      "    Hit Rate:   0.47\n",
      "   Sai : Chinese != Indian\n",
      "iteration # 200000 of 200000\n",
      "   Avg. loss:   1.33\n",
      "    Hit Rate:   0.58\n",
      "   Hunter : Scottish == Scottish\n",
      " \n",
      "Classification Accuracy: 56.81%\n",
      " \n",
      "Confusion Matrix\n",
      "      |----------------------- Predicted ------------------------- ...\n",
      "Actual|Czech |German|Arabic|Japane|Chines|Vietna|Russia|French|Irish |Englis|Indian|Pakist|Spanis|Greek |Italia|Portug|Scotti|Dutch |Korean|Polish|\n",
      "======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|\n",
      "Czech |  3580|  1400|   300|   320|    60|   140|   300|   240|   260|   720|   200|   140|   360|   140|    80|   160|   420|   280|    40|  1240|\n",
      "German|   560|  7940|   540|   100|   160|   180|   120|   520|   560|  1040|   160|   140|   160|   220|    80|    80|  1040|   500|   120|   260|\n",
      "Arabic|   260|  1900| 25020|   680|  1060|     0|     0|     0|  2380|     0|  2220|  2100|   320|  1040|  1080|   740|   700|     0|   500|     0|\n",
      "Japane|   120|   140|   680| 14260|   160|   160|    80|   240|   100|   180|   260|   740|   420|   120|   680|   400|   140|   200|   240|   500|\n",
      "Chines|     0|    20|     0|     0|  3700|   640|     0|     0|     0|    20|    60|    60|    20|     0|     0|    40|     0|     0|   800|     0|\n",
      "Vietna|     0|     0|     0|     0|   200|  1180|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|    80|     0|\n",
      "Russia|  5140|  6840|  2400|  2760|   800|   840|123320|  1780|  6300|  5200|   740|  2460|   840|  3400|  1740|  1060|  6880|  1340|   960| 13360|\n",
      "French|    20|   720|   100|    40|    20|    20|   120|  2460|   240|   560|   120|    80|     0|   260|    40|    60|   460|    60|   100|    60|\n",
      "Irish |    80|   260|   100|    20|    40|     0|    20|   100|  3180|   200|    60|    60|     0|    80|    20|    20|   340|    40|    20|     0|\n",
      "Englis|  2120|  8560|  2680|   500|   720|   760|  1300|  4460|  7360| 21140|  1480|   800|   600|  2080|   240|  1140| 13180|  2340|   740|  1160|\n",
      "Indian|    60|   160|    20|    20|    40|    40|     0|    20|    40|    40|  1140|   180|     0|     0|    60|    60|    40|    40|    40|     0|\n",
      "Pakist|   120|    40|   260|   180|    80|    80|    40|    20|   160|   160|   400|  2440|    40|    40|   400|    20|   200|    60|    20|   100|\n",
      "Spanis|   140|   180|   100|   180|    60|    80|     0|   180|   220|   120|   180|    60|  2480|   220|   760|   740|   100|    40|    40|    80|\n",
      "Greek |    20|     0|    20|    60|     0|     0|    20|    20|     0|    20|     0|     0|    40|  3620|    60|    80|    40|     0|     0|    60|\n",
      "Italia|   220|   220|   160|   400|   100|    80|    80|   380|   300|   180|   440|   620|  1140|   180|  8400|   900|   260|    20|    40|    60|\n",
      "Portug|    60|     0|     0|    20|     0|    20|     0|    20|    60|     0|    20|    40|    80|    40|    60|  1020|    20|    20|     0|     0|\n",
      "Scotti|    40|   160|     0|     0|   100|     0|     0|     0|   200|   160|    20|     0|    20|    20|     0|     0|  1260|    20|     0|     0|\n",
      "Dutch |   240|   840|   140|    40|    80|    80|    20|   320|   220|   340|   120|    80|   140|    80|     0|    60|   480|  2460|   100|   100|\n",
      "Korean|    20|     0|    20|     0|   300|   160|     0|     0|     0|    40|     0|     0|     0|     0|     0|     0|     0|     0|  1320|    20|\n",
      "Polish|   200|   120|    40|    60|     0|    20|     0|    20|    60|   100|     0|    20|     0|     0|     0|    40|    20|    20|    20|  2040|\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # declare regex for files containing names\n",
    "    fnames = 'data/names/*.txt'\n",
    "\n",
    "    # assemble sequence of valid ASCII characters\n",
    "    # that can occur in a name\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    n_letters = len(all_letters)\n",
    "\n",
    "    # Build the category_lines dictionary, a list of names per language\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "\n",
    "    nfiles = 0\n",
    "    for filename in glob.glob(fnames):\n",
    "        # basename of file is the lanquage\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        # add category (i.e. language) to list\n",
    "        all_categories.append(category)\n",
    "        # add names to dictionary, indexed by language\n",
    "        lines = readLines(filename, all_letters)\n",
    "        category_lines[category] = lines\n",
    "        nfiles += 1\n",
    "    if(nfiles == 0):\n",
    "        print(\"No files found for regular expression (\"+fnames+\")\")\n",
    "        sys.exit(-1)\n",
    "        \n",
    "    # count number of languages (i.e. classes)\n",
    "    n_categories = len(all_categories)\n",
    "\n",
    "    # write categories to csv file\n",
    "    with open('all_categories.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([all_categories])\n",
    "\n",
    "    # 4. create instance of the RNN\n",
    "    n_input_neurons = n_letters\n",
    "    n_hidden_neurons = 256\n",
    "    n_output_neurons = n_categories\n",
    "    MyRNN = RNN(n_input_neurons, n_hidden_neurons, n_output_neurons)\n",
    "    \n",
    "    # 5. load checkpoint, if available\n",
    "    \n",
    "    # 6. set training parameters\n",
    "    my_criterion = nn.NLLLoss()\n",
    "    my_learn_rate = 0.005\n",
    "    n_iters = 200000\n",
    "    print_every = 100000\n",
    "    avg_loss = 0.00\n",
    "    hit_rate = 0.00\n",
    "    dhit = 1.00 / float(print_every)\n",
    "    all_avg_losses = []\n",
    "    all_hit_rates = []\n",
    "    \n",
    "    for i in range(1, n_iters + 1):\n",
    "# TO DO:\n",
    "        # 7. train on random feature-label pair\n",
    "        category, line, category_tensor, line_tensor =\\\n",
    "                  randomTrainingExample(all_categories, category_lines, all_letters)\n",
    "        output, loss = train(MyRNN, category_tensor, line_tensor, my_criterion, my_learn_rate)\n",
    "        avg_loss += loss\n",
    "        guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "\n",
    "        if(guess == category):\n",
    "            hit_rate += dhit\n",
    "            \n",
    "        # periodically report progress\n",
    "        if i % print_every == 0:\n",
    "            avg_loss /= float(print_every)\n",
    "            print(\"iteration # \" + str(i) + \" of \" + str(n_iters))            \n",
    "            print(\"   Avg. loss: {:6.2f}\".format(avg_loss))\n",
    "            print(\"    Hit Rate: {:6.2f}\".format(hit_rate))\n",
    "            if(guess == category):\n",
    "                print(\"   \" + line + \" : \" + guess + \" == \" + category)\n",
    "            else:\n",
    "                print(\"   \" + line + \" : \" + guess + \" != \" + category)\n",
    "            all_avg_losses.append(avg_loss)\n",
    "            all_hit_rates.append(hit_rate)\n",
    "            avg_loss = 0.00\n",
    "            hit_rate = 0.00\n",
    "            \n",
    "        # 8. Save a checkpoint\n",
    "            \n",
    "    fig,ax1 = plt.subplots()\n",
    "    ax1.plot(all_avg_losses)\n",
    "    ax1.set_ylabel(\"Loss Function\", color=\"r\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(all_hit_rates)\n",
    "    ax2.set_ylabel(\"Success Rate\", color=\"b\")\n",
    "    plt.xlabel(\"Training Iteration\")\n",
    "    plt.savefig(\"convergence.png\", dpi=100)\n",
    "    # plt.show()    \n",
    "    plt.close()\n",
    "\n",
    "    # test the skill of the model\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    confusion_matrix = np.zeros((n_categories,n_categories))\n",
    "    for actual_category in all_categories:\n",
    "        cat_idx = all_categories.index(actual_category)\n",
    "        for line in category_lines[actual_category]:\n",
    "            line_tensor = textToTensor(line, all_letters)\n",
    "            output = predict(MyRNN, line_tensor)\n",
    "            guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "            \n",
    "            # update confusion matrix\n",
    "            for i in range(0,n_categories):\n",
    "                confusion_matrix[cat_idx,guess_idx] += 1          \n",
    "            total += 1\n",
    "            correct += int(cat_idx == guess_idx)\n",
    "\n",
    "    # report results\n",
    "    print(' ')\n",
    "    print('Classification Accuracy: {:.2f}%'.format(100 * correct / total))\n",
    "    print(' ')\n",
    "    print('Confusion Matrix')\n",
    "    print('      |----------------------- Predicted ------------------------- ...')\n",
    "    print('Actual|',end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "    print('')\n",
    "    print('======|', end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('======|', end='')\n",
    "    print('')          \n",
    "    for i in range(0,n_categories):\n",
    "        print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "        for j in range(0,n_categories):\n",
    "            cm = int(confusion_matrix[i,j])\n",
    "            print('{:6d}|'.format(cm),end='')\n",
    "        print('')\n",
    "\n",
    "# launch the main program\n",
    "main()\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 250000 of 500000\n",
      "   Avg. loss:   1.49\n",
      "    Hit Rate:   0.53\n",
      "   Bhrighde : English != Irish\n",
      "iteration # 500000 of 500000\n",
      "   Avg. loss:   1.24\n",
      "    Hit Rate:   0.60\n",
      "   Poirier : French == French\n",
      " \n",
      "Classification Accuracy: 52.63%\n",
      " \n",
      "Confusion Matrix\n",
      "      |----------------------- Predicted ------------------------- ...\n",
      "Actual|Czech |German|Arabic|Japane|Chines|Vietna|Russia|French|Irish |Englis|Indian|Pakist|Spanis|Greek |Italia|Portug|Scotti|Dutch |Korean|Polish|\n",
      "======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|======|\n",
      "Czech |  4200|  1220|   140|   340|    20|   100|   300|   520|   300|   240|   340|   180|   160|   140|   100|   340|   220|   460|    20|  1040|\n",
      "German|   620|  7540|   380|   180|   120|   160|   120|  1220|   240|   400|   460|   140|   200|   220|   100|   100|   380|  1540|   140|   220|\n",
      "Arabic|   820|   800| 20060|  3300|     0|   320|   580|   940|   440|     0|  3760|  2160|   680|  2240|   640|  1320|     0|  1500|     0|   440|\n",
      "Japane|   440|    20|   300| 14460|    40|   160|   160|   400|    20|    20|   620|   420|   200|   460|  1100|   320|    40|   120|   160|   360|\n",
      "Chines|     0|    20|     0|    60|  3760|   440|    20|     0|    40|    20|   180|    40|     0|     0|     0|    60|    20|    20|   660|    20|\n",
      "Vietna|     0|     0|     0|     0|   280|  1000|     0|     0|     0|     0|    60|     0|     0|     0|     0|     0|     0|     0|   120|     0|\n",
      "Russia| 14700|  9740|  1480|  2280|   800|   400|117800|  5900|  3180|  1740|  2780|  1720|  1020|  3460|  2160|  1540|  3080|  5420|   480|  8480|\n",
      "French|    20|   580|   100|    40|    20|    40|    80|  3300|   120|   140|   200|    40|    40|   200|    80|   100|   160|   200|    60|    20|\n",
      "Irish |    60|   280|    40|    20|    40|     0|   220|   420|  2500|   100|   220|    60|    20|    60|     0|    80|   240|   260|     0|    20|\n",
      "Englis|  2600| 10920|  2020|   660|   520|   600|  1320| 10560|  4820| 13280|  3540|   760|   700|  2140|   340|  1500|  5960|  8140|   400|  2580|\n",
      "Indian|    80|    40|    40|    20|    20|     0|     0|    20|    40|     0|  1420|   140|    20|     0|    20|    60|     0|    40|    40|     0|\n",
      "Pakist|   120|   100|   200|   380|   100|    40|    60|    80|   120|    80|   760|  1940|    20|    60|   420|    80|   140|    60|    20|    80|\n",
      "Spanis|   120|    80|     0|   140|    80|    20|     0|   400|    60|   100|   260|    60|  2600|   220|   580|  1060|    40|   120|     0|    20|\n",
      "Greek |    40|    40|    20|    40|     0|     0|     0|   100|     0|     0|     0|    20|    60|  3420|    20|   220|     0|    60|     0|    20|\n",
      "Italia|   220|   200|    40|   260|    80|    40|    60|   540|    60|   100|   720|   320|  1100|   140|  8860|  1240|   100|    40|    20|    40|\n",
      "Portug|    20|     0|    20|    40|     0|     0|     0|    40|     0|     0|    40|    40|   160|    60|    40|  1020|     0|     0|     0|     0|\n",
      "Scotti|    20|   240|     0|     0|    20|     0|    20|   140|   120|   100|    40|     0|    20|    20|     0|    20|  1160|    60|     0|    20|\n",
      "Dutch |   380|   620|    60|    20|   120|    60|     0|   400|   100|    40|   200|    80|    60|   160|    40|    20|   160|  3280|    40|   100|\n",
      "Korean|    40|     0|    20|     0|   260|   120|    40|     0|    20|     0|    20|     0|     0|     0|     0|     0|     0|    20|  1320|    20|\n",
      "Polish|   280|   120|    40|    20|    40|     0|    60|    60|    20|    20|    20|     0|    20|    20|     0|     0|    40|    20|    20|  1980|\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # declare regex for files containing names\n",
    "    fnames = 'data/names/*.txt'\n",
    "\n",
    "    # assemble sequence of valid ASCII characters\n",
    "    # that can occur in a name\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    n_letters = len(all_letters)\n",
    "\n",
    "    # Build the category_lines dictionary, a list of names per language\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "\n",
    "    nfiles = 0\n",
    "    for filename in glob.glob(fnames):\n",
    "        # basename of file is the lanquage\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        # add category (i.e. language) to list\n",
    "        all_categories.append(category)\n",
    "        # add names to dictionary, indexed by language\n",
    "        lines = readLines(filename, all_letters)\n",
    "        category_lines[category] = lines\n",
    "        nfiles += 1\n",
    "    if(nfiles == 0):\n",
    "        print(\"No files found for regular expression (\"+fnames+\")\")\n",
    "        sys.exit(-1)\n",
    "        \n",
    "    # count number of languages (i.e. classes)\n",
    "    n_categories = len(all_categories)\n",
    "\n",
    "    # write categories to csv file\n",
    "    with open('all_categories.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([all_categories])\n",
    "\n",
    "    # 4. create instance of the RNN\n",
    "    n_input_neurons = n_letters\n",
    "    n_hidden_neurons = 256\n",
    "    n_output_neurons = n_categories\n",
    "    MyRNN = RNN(n_input_neurons, n_hidden_neurons, n_output_neurons)\n",
    "    \n",
    "    # 5. load checkpoint, if available\n",
    "    \n",
    "    # 6. set training parameters\n",
    "    my_criterion = nn.NLLLoss()\n",
    "    my_learn_rate = 0.005\n",
    "    n_iters = 500000\n",
    "    print_every = 250000\n",
    "    avg_loss = 0.00\n",
    "    hit_rate = 0.00\n",
    "    dhit = 1.00 / float(print_every)\n",
    "    all_avg_losses = []\n",
    "    all_hit_rates = []\n",
    "    \n",
    "    for i in range(1, n_iters + 1):\n",
    "        category, line, category_tensor, line_tensor =\\\n",
    "                  randomTrainingExample(all_categories, category_lines, all_letters)\n",
    "        output, loss = train(MyRNN, category_tensor, line_tensor, my_criterion, my_learn_rate)\n",
    "        avg_loss += loss\n",
    "        guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "\n",
    "        if(guess == category):\n",
    "            hit_rate += dhit\n",
    "            \n",
    "        # periodically report progress\n",
    "        if i % print_every == 0:\n",
    "            avg_loss /= float(print_every)\n",
    "            print(\"iteration # \" + str(i) + \" of \" + str(n_iters))            \n",
    "            print(\"   Avg. loss: {:6.2f}\".format(avg_loss))\n",
    "            print(\"    Hit Rate: {:6.2f}\".format(hit_rate))\n",
    "            if(guess == category):\n",
    "                print(\"   \" + line + \" : \" + guess + \" == \" + category)\n",
    "            else:\n",
    "                print(\"   \" + line + \" : \" + guess + \" != \" + category)\n",
    "            all_avg_losses.append(avg_loss)\n",
    "            all_hit_rates.append(hit_rate)\n",
    "            avg_loss = 0.00\n",
    "            hit_rate = 0.00\n",
    "            \n",
    "        # 8. Save a checkpoint\n",
    "            \n",
    "    fig,ax1 = plt.subplots()\n",
    "    ax1.plot(all_avg_losses)\n",
    "    ax1.set_ylabel(\"Loss Function\", color=\"r\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(all_hit_rates)\n",
    "    ax2.set_ylabel(\"Success Rate\", color=\"b\")\n",
    "    plt.xlabel(\"Training Iteration\")\n",
    "    plt.savefig(\"convergence.png\", dpi=100)\n",
    "    # plt.show()    \n",
    "    plt.close()\n",
    "\n",
    "    # test the skill of the model\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    confusion_matrix = np.zeros((n_categories,n_categories))\n",
    "    for actual_category in all_categories:\n",
    "        cat_idx = all_categories.index(actual_category)\n",
    "        for line in category_lines[actual_category]:\n",
    "            line_tensor = textToTensor(line, all_letters)\n",
    "            output = predict(MyRNN, line_tensor)\n",
    "            guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "            \n",
    "            # update confusion matrix\n",
    "            for i in range(0,n_categories):\n",
    "                confusion_matrix[cat_idx,guess_idx] += 1          \n",
    "            total += 1\n",
    "            correct += int(cat_idx == guess_idx)\n",
    "\n",
    "    # report results\n",
    "    print(' ')\n",
    "    print('Classification Accuracy: {:.2f}%'.format(100 * correct / total))\n",
    "    print(' ')\n",
    "    print('Confusion Matrix')\n",
    "    print('      |----------------------- Predicted ------------------------- ...')\n",
    "    print('Actual|',end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "    print('')\n",
    "    print('======|', end='')\n",
    "    for i in range(0,n_categories):\n",
    "          print('======|', end='')\n",
    "    print('')          \n",
    "    for i in range(0,n_categories):\n",
    "        print('{:6s}|'.format(all_categories[i][0:6]), end='')\n",
    "        for j in range(0,n_categories):\n",
    "            cm = int(confusion_matrix[i,j])\n",
    "            print('{:6d}|'.format(cm),end='')\n",
    "        print('')\n",
    "\n",
    "# launch the main program\n",
    "main()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_torch]",
   "language": "python",
   "name": "conda-env-conda_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
