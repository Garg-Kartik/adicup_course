{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint (mnist_names_model.pkl)\n",
      "name = Popescu, origin = Czech\n",
      "name = Fernandez, origin = Portuguese\n",
      "name = Velenzuela, origin = German\n",
      "name = Lovecraft, origin = French\n",
      "name = Chambers, origin = English\n",
      "name = Davies, origin = Portuguese\n",
      "name = Paltrowski, origin = Polish\n",
      "name = Sargiannis, origin = Greek\n",
      "name = Ovechkin, origin = Russian\n",
      "name = Fapp, origin = German\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import glob\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "\n",
    "# various helper functions\n",
    "from torch_name_classifier_helpers import readLines\n",
    "from torch_name_classifier_helpers import categoryFromOutput\n",
    "from torch_name_classifier_helpers import textToTensor\n",
    "\n",
    "# declare RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_layer, hidden_layer):\n",
    "        combined_layer = torch.cat((input_layer,hidden_layer), 1)\n",
    "        hidden_layer = self.i2h(combined_layer)\n",
    "        output_layer = self.i2o(combined_layer)\n",
    "        output_layer = self.softmax(output_layer)\n",
    "        return output_layer, hidden_layer\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,self.hidden_size)\n",
    "\n",
    "def predict(the_rnn, line_tensor):\n",
    "    hidden_layer = the_rnn.initHidden()\n",
    "\n",
    "    for i in range(list(line_tensor.size())[0]):\n",
    "        output_layer,hidden_layer = the_rnn(line_tensor[i], hidden_layer)\n",
    "\n",
    "    return output_layer\n",
    "        \n",
    "\n",
    "def main():\n",
    "    # declare regex for files containing names\n",
    "    fnames = 'data/names/*.txt'\n",
    "\n",
    "    # assemble sequence of valid ASCII characters\n",
    "    # that can occur in a name\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    n_letters = len(all_letters)\n",
    "\n",
    "    # Build the category_lines dictionary, a list of names per language\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "\n",
    "    nfiles = 0\n",
    "    for filename in glob.glob(fnames):\n",
    "        # basename of file is the lanquage\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        # add category (i.e. language) to list\n",
    "        all_categories.append(category)\n",
    "        # add names to dictionary, indexed by language\n",
    "        lines = readLines(filename, all_letters)\n",
    "        category_lines[category] = lines\n",
    "        nfiles += 1\n",
    "    if(nfiles == 0):\n",
    "        print(\"No files found for regular expression (\"+fnames+\")\")\n",
    "        sys.exit(-1)\n",
    "        \n",
    "    # count number of languages (i.e. classes)\n",
    "    n_categories = len(all_categories)\n",
    "\n",
    "    # write categories to csv file\n",
    "    with open('all_categories.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([all_categories])\n",
    "\n",
    "    # 4. create instance of the RNN\n",
    "    n_input_neurons = n_letters\n",
    "    n_hidden_neurons = 256\n",
    "    n_output_neurons = n_categories\n",
    "    MyRNN = RNN(n_input_neurons, n_hidden_neurons, n_output_neurons)\n",
    "    \n",
    "    # 5. load checkpoint, if available\n",
    "    le='mnist_names_model.pkl'\n",
    "    if(os.path.isfile(checkpoint_file)==True):\n",
    "        print(\"Resuming from checkpoint (\"+checkpoint_file+\")\")\n",
    "        MyRNN.load_state_dict(torch.load(checkpoint_file))\n",
    "        MyRNN.eval()\n",
    "    \n",
    "    names = [\"Popescu\",\"Fernandez\",\"Velenzuela\",\"Lovecraft\",\"Chambers\",\"Davies\",\"Paltrowski\",\"Sargiannis\",\"Ovechkin\",\"Fapp\"]\n",
    "    for name in names:\n",
    "        line_tensor = textToTensor(name, all_letters)\n",
    "        output = predict(MyRNN, line_tensor)\n",
    "        guess, guess_idx = categoryFromOutput(output, all_categories)\n",
    "        print(\"name = {:s}, origin = {:s}\".format(name, guess))\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_torch]",
   "language": "python",
   "name": "conda-env-conda_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
